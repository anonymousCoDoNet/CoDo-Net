#!/usr/bin/env python3
"""
åŸºäºCSVæ•°æ®é›†çš„landmarkæå–è„šæœ¬
ä»successful_preprocessed_metadata.csvä¸­è¯»å–è§†é¢‘ä¿¡æ¯å¹¶æå–68ä¸ªé¢éƒ¨å…³é”®ç‚¹
"""

import cv2
import os
import numpy as np
import torch
from tqdm import tqdm
import os.path as osp
import sys
from skimage import io
import face_alignment
from glob import glob
import json
import argparse
import pandas as pd
from pathlib import Path

def detect_save_landmark_68_csv(args):
    """ä»CSVæ–‡ä»¶ä¸­è¯»å–è§†é¢‘ä¿¡æ¯å¹¶æå–landmark"""
    csv_path = args.csv_path
    video_root = args.video_root
    out_dir = args.out_dir
    
    # è¯»å–CSVæ–‡ä»¶
    print(f"ğŸ“– è¯»å–CSVæ–‡ä»¶: {csv_path}")
    df = pd.read_csv(csv_path)
    print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {len(df)}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    print(f"ğŸ“Š æ ‡ç­¾åˆ†å¸ƒ: {df['label'].value_counts().to_dict()}")
    if 'category' in df.columns:
        print(f"ğŸ“Š ç±»åˆ«åˆ†å¸ƒ: {df['category'].value_counts().to_dict()}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(out_dir, exist_ok=True)
    
    # ç»Ÿè®¡å¤„ç†ç»“æœ
    processed_count = 0
    skipped_count = 0
    failed_count = 0
    
    print("ğŸš€ å¼€å§‹æå–landmark...")
    
    for idx, row in tqdm(df.iterrows(), total=len(df), desc="å¤„ç†è§†é¢‘"):
        # è·å–è§†é¢‘è·¯å¾„
        if args.use_full_paths:
            video_path = row['original_path']
        else:
            video_path = osp.join(video_root, row['original_path'])
        
        # ç”Ÿæˆè¾“å‡ºè·¯å¾„
        video_id = row['video_id']
        segment_id = row['segment_id']
        out_path = osp.join(out_dir, f"{video_id}_{segment_id}.json")
        
        # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        if osp.exists(out_path) and not args.force:
            skipped_count += 1
            continue
        
        # æ£€æŸ¥è¾“å…¥è§†é¢‘æ˜¯å¦å­˜åœ¨
        if not osp.exists(video_path):
            print(f"âš ï¸  è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
            failed_count += 1
            continue
        
        try:
            # åˆ›å»ºè¾“å‡ºç›®å½•
            os.makedirs(osp.dirname(out_path), exist_ok=True)
            
            # è¯»å–è§†é¢‘å¸§
            frames = []
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                print(f"âš ï¸  æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")
                failed_count += 1
                continue
                
            while cap.isOpened():
                ret, frame = cap.read()
                if not ret:
                    break
                frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
            cap.release()
            
            if len(frames) == 0:
                print(f"âš ï¸  è§†é¢‘ä¸ºç©º: {video_path}")
                failed_count += 1
                continue
            
            frames = np.asarray(frames)
            
            # æå–landmark
            landmarks = {}
            for i in range(len(frames)):
                frame = frames[i]
                landmark = fa.get_landmarks(frame)
                if (landmark is not None) and (len(landmark) != 0):
                    landmark = landmark[0]
                    landmark = landmark.tolist()
                else:
                    landmark = None
                
                img_name = f'{i:04d}.jpg'
                landmarks[img_name] = landmark
            
            # ä¿å­˜landmark
            with open(out_path, 'w') as f:
                json.dump(landmarks, f, indent=2)
            
            processed_count += 1
            
        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥ {video_path}: {e}")
            failed_count += 1
            continue
    
    # è¾“å‡ºç»Ÿè®¡ç»“æœ
    print(f"\nğŸ“Š å¤„ç†å®Œæˆ:")
    print(f"  âœ… æˆåŠŸå¤„ç†: {processed_count} ä¸ªè§†é¢‘")
    print(f"  â­ï¸  è·³è¿‡: {skipped_count} ä¸ªè§†é¢‘")
    print(f"  âŒ å¤±è´¥: {failed_count} ä¸ªè§†é¢‘")
    print(f"  ğŸ“ è¾“å‡ºç›®å½•: {out_dir}")

def create_file_list_from_csv(csv_path, output_file, video_root="", use_full_paths=False):
    """ä»CSVæ–‡ä»¶åˆ›å»ºæ–‡ä»¶åˆ—è¡¨"""
    df = pd.read_csv(csv_path)
    
    with open(output_file, 'w') as f:
        for idx, row in df.iterrows():
            if use_full_paths:
                video_path = row['original_path']
            else:
                video_path = osp.join(video_root, row['original_path'])
            
            # æ ¼å¼: video_path label
            f.write(f"{video_path} {row['label']}\n")
    
    print(f"ğŸ“ æ–‡ä»¶åˆ—è¡¨å·²ä¿å­˜åˆ°: {output_file}")
    print(f"ğŸ“Š åŒ…å« {len(df)} ä¸ªè§†é¢‘")

def main():
    parser = argparse.ArgumentParser(
        description='åŸºäºCSVæ•°æ®é›†çš„landmarkæå–è„šæœ¬',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # ä¸»è¦å‚æ•°
    parser.add_argument('--csv_path', type=str, required=True,
                       help='CSVæ•°æ®é›†æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--video_root', type=str, default='',
                       help='è§†é¢‘æ ¹ç›®å½• (å¦‚æœä¸ä½¿ç”¨å®Œæ•´è·¯å¾„)')
    parser.add_argument('--out_dir', type=str, required=True,
                       help='landmarkè¾“å‡ºç›®å½•')
    parser.add_argument('--use_full_paths', action='store_true',
                       help='ä½¿ç”¨CSVä¸­çš„å®Œæ•´è·¯å¾„è€Œä¸æ˜¯ç›¸å¯¹è·¯å¾„')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--face_detector', type=str, 
                       default='checkpoints/Resnet50_Final.pth',
                       help='äººè„¸æ£€æµ‹å™¨è·¯å¾„')
    parser.add_argument('--face_predictor', type=str,
                       default='checkpoints/2DFAN4-cd938726ad.zip',
                       help='landmarké¢„æµ‹å™¨è·¯å¾„')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--force', action='store_true',
                       help='å¼ºåˆ¶é‡æ–°å¤„ç†å·²å­˜åœ¨çš„æ–‡ä»¶')
    parser.add_argument('--create_file_list', type=str, default='',
                       help='åˆ›å»ºæ–‡ä»¶åˆ—è¡¨å¹¶ä¿å­˜åˆ°æŒ‡å®šè·¯å¾„')
    parser.add_argument('--ffmpeg', type=str, default='/usr/bin/ffmpeg',
                       help='ffmpegè·¯å¾„')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not osp.exists(args.csv_path):
        print(f"âŒ CSVæ–‡ä»¶ä¸å­˜åœ¨: {args.csv_path}")
        return
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
    if not osp.exists(args.face_detector):
        print(f"âŒ äººè„¸æ£€æµ‹å™¨ä¸å­˜åœ¨: {args.face_detector}")
        print("è¯·ä¸‹è½½RetinaFaceæ¨¡å‹åˆ°checkpoints/ç›®å½•")
        return
    
    # åˆ›å»ºæ–‡ä»¶åˆ—è¡¨ (å¦‚æœè¯·æ±‚)
    if args.create_file_list:
        create_file_list_from_csv(
            args.csv_path, 
            args.create_file_list, 
            args.video_root, 
            args.use_full_paths
        )
        return
    
    # åˆå§‹åŒ–face alignment
    print("ğŸ”§ åˆå§‹åŒ–Face Alignment...")
    try:
        global fa
        fa = face_alignment.FaceAlignment(
            face_alignment.LandmarksType.TWO_D,
            face_detector='retinaface',
            device='cuda' if torch.cuda.is_available() else 'cpu',
            face_detector_kwargs={'path_to_detector': args.face_detector}
        )
        print("âœ… Face Alignmentåˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ Face Alignmentåˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # å¼€å§‹å¤„ç†
    detect_save_landmark_68_csv(args)

if __name__ == '__main__':
    main()
